"""
ORA Empathic Agent - HUME VOICE ONLY INTEGRATION
Keep ORA's emotion detection + Add Hume's empathic voice
Simple integration for Make.com workflow
"""
import os
import json
import time
import requests
import base64
from datetime import datetime
from flask import Flask, request, jsonify, render_template
from flask_cors import CORS

app = Flask(__name__, template_folder="templates", static_folder="static")
CORS(app)

# Hume API Configuration (for voice only)
HUME_API_KEY = os.getenv("HUME_API_KEY", "")
HUME_TTS_URL = "https://api.hume.ai/v0/tts/batches"

# Make.com webhook URLs (your existing setup)
MAKE_WEBHOOKS = {
    "stress_intervention": os.getenv("MAKE_STRESS_WEBHOOK", ""),
    "anxiety_support": os.getenv("MAKE_ANXIETY_WEBHOOK", ""),
    "depression_care": os.getenv("MAKE_DEPRESSION_WEBHOOK", ""),
    "crisis_alert": os.getenv("MAKE_CRISIS_WEBHOOK", ""),
    "wellness_check": os.getenv("MAKE_WELLNESS_WEBHOOK", ""),
    "mood_tracking": os.getenv("MAKE_MOOD_WEBHOOK", ""),
    "proactive_care": os.getenv("MAKE_PROACTIVE_WEBHOOK", "")
}

# OpenAI fallback (your existing setup)
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")

# In-memory stores
conversations = {}
user_profiles = {}
action_history = {}

# Check if Hume is available
HUME_AVAILABLE = bool(HUME_API_KEY)

if HUME_AVAILABLE:
    print("‚úÖ Hume Voice configured successfully")
else:
    print("‚ö†Ô∏è Hume Voice not configured - will use text responses only")

class EmpathicAgent:
    """Your existing empathic agent logic - KEEP AS IS"""
    
    @staticmethod
    def analyze_emotional_context(text, emotion, user_id):
        context = {
            "text": text.lower(),
            "emotion": emotion.lower(),
            "user_id": user_id,
            "timestamp": datetime.now().isoformat(),
            "urgency": "normal"
        }
        
        # Crisis detection (your existing logic)
        if any(word in context["text"] for word in ["crisis", "suicide", "hurt myself", "end it all"]):
            context["urgency"] = "critical"
            context["action_type"] = "crisis_intervention"
        elif emotion.lower() in ["anxious", "panic", "overwhelmed"]:
            if any(word in context["text"] for word in ["can't breathe", "panic attack", "overwhelming"]):
                context["urgency"] = "high"
            context["action_type"] = "anxiety_support"
        elif emotion.lower() in ["sad", "depressed", "hopeless"]:
            if any(word in context["text"] for word in ["alone", "nobody", "isolated"]):
                context["urgency"] = "high"
            context["action_type"] = "depression_care"
        elif emotion.lower() in ["stressed", "frustrated"]:
            context["action_type"] = "stress_intervention"
        else:
            context["action_type"] = "wellness_check"
        
        return context
    
    @staticmethod
    def execute_proactive_actions(context):
        actions_taken = []
        webhook_url = MAKE_WEBHOOKS.get(context["action_type"])
        
        if not webhook_url:
            return actions_taken
        
        payload = {
            "trigger": context["action_type"],
            "user_id": context["user_id"],
            "emotion": context["emotion"],
            "urgency": context["urgency"],
            "text_snippet": context["text"][:200],
            "timestamp": context["timestamp"]
        }
        
        try:
            response = requests.post(webhook_url, json=payload, timeout=10)
            
            if response.status_code == 200:
                actions_taken.append(f"‚úÖ Triggered {context['action_type']} workflow")
                
                if context["user_id"] not in action_history:
                    action_history[context["user_id"]] = []
                
                action_history[context["user_id"]].append({
                    "timestamp": context["timestamp"],
                    "action_type": context["action_type"],
                    "urgency": context["urgency"],
                    "status": "executed"
                })
            else:
                actions_taken.append(f"‚ùå Failed to trigger {context['action_type']}")
                
        except Exception as e:
            actions_taken.append(f"‚ùå Webhook error: {str(e)}")
        
        return actions_taken

class ORAEmotionDetection:
    """Your existing ORA emotion detection - KEEP AS IS"""
    
    @staticmethod
    def detect_emotion_from_text(text):
        """Your existing emotion detection logic"""
        
        emotion_keywords = {
            "anxious": ["worried", "anxious", "nervous", "scared", "panic", "stress", "overwhelmed"],
            "sad": ["sad", "depressed", "down", "hopeless", "lonely", "empty", "crying"],
            "angry": ["angry", "mad", "furious", "irritated", "frustrated", "annoyed"],
            "happy": ["happy", "joy", "excited", "great", "wonderful", "amazing", "fantastic"],
            "calm": ["calm", "peaceful", "relaxed", "serene", "tranquil"],
            "confused": ["confused", "lost", "unclear", "don't understand"],
            "neutral": []
        }
        
        text_lower = text.lower()
        detected_emotions = {}
        
        for emotion, keywords in emotion_keywords.items():
            score = sum(1 for keyword in keywords if keyword in text_lower)
            if score > 0:
                detected_emotions[emotion] = min(score * 0.4, 1.0)
        
        if not detected_emotions:
            detected_emotions["neutral"] = 0.8
        
        return detected_emotions
    
    @staticmethod
    def generate_empathic_response(text, emotions, user_id):
        """Your existing response generation - enhanced for voice"""
        
        dominant_emotion = max(emotions.items(), key=lambda x: x[1])[0] if emotions else "neutral"
        
        # Enhanced empathic response templates (optimized for voice)
        response_templates = {
            "anxious": [
                "I can sense the worry in your voice. Take a deep breath with me. You're safe here, and we can work through this together.",
                "That anxiety sounds overwhelming. Let's slow down for a moment. What's the most pressing thing on your mind right now?",
                "I hear the stress in what you're sharing. Remember, you don't have to carry this burden alone. I'm here with you.",
                "Your anxiety is completely understandable. Let's take this one step at a time. What would help you feel a little more grounded right now?"
            ],
            "sad": [
                "I can feel the sadness in your words. It's okay to feel this way, and I'm here to listen without judgment.",
                "That sounds really painful. Your feelings are valid, and you don't have to go through this alone.",
                "I hear the heaviness in your voice. Sometimes just being heard can help lighten the load a little.",
                "I'm sitting with you in this sadness. You don't have to be strong right now, just be present with me."
            ],
            "angry": [
                "I can sense the frustration in your words. It's completely understandable to feel this way. What's driving these feelings?",
                "That anger sounds justified. Let's talk through what's happening and find a way forward together.",
                "I hear how upset you are. Your feelings matter, and I'm here to help you process them safely.",
                "That frustration is real and valid. Let's channel this energy into understanding what you need right now."
            ],
            "happy": [
                "I love hearing the joy in your voice! It's wonderful that you're feeling good. What's bringing you this happiness today?",
                "Your happiness is contagious! I'm so glad you're having a good moment. Tell me more about what's going well.",
                "That's fantastic! It's beautiful to hear such positive energy. I'm here to celebrate these good moments with you.",
                "I can hear the smile in your voice! It's wonderful when life feels bright. What's making today special for you?"
            ],
            "neutral": [
                "I'm here and listening. How are you feeling right now? What's on your mind?",
                "Thank you for sharing with me. I'm here to support you in whatever way you need.",
                "I'm glad you reached out. What would be most helpful for you right now?",
                "I'm present with you. Take your time, what's going on for you today?"
            ]
        }
        
        # Handle specific questions (your existing logic)
        if "?" in text:
            if any(word in text.lower() for word in ["what", "how", "why", "when", "where"]):
                if any(math_word in text.lower() for math_word in ["2+2", "plus", "minus", "times", "divided", "math", "calculate"]):
                    if "2+2" in text or "2 + 2" in text:
                        return "Two plus two equals four! I'm happy to help with any questions you have, whether they're mathematical or about how you're feeling."
                    else:
                        return "I'd be happy to help with that calculation! I'm here for both practical questions and emotional support."
                elif "joke" in text.lower():
                    return "Here's a gentle one for you: Why don't scientists trust atoms? Because they make up everything! I hope that brought a little smile to your face. How are you feeling today?"
                elif any(word in text.lower() for word in ["time", "date", "day"]):
                    return f"It's {datetime.now().strftime('%A, %B %d, %Y at %I:%M %p')}. How are you feeling today? Is there anything on your mind?"
        
        # Get appropriate response template
        templates = response_templates.get(dominant_emotion, response_templates["neutral"])
        
        # Simple response selection
        import random
        response = random.choice(templates)
        
        return response

class HumeVoiceGenerator:
    """NEW: Hume voice generation only"""
    
    @staticmethod
    def generate_hume_voice(text, emotions, user_id="default"):
        """Generate empathic voice using Hume TTS API"""
        
        if not HUME_AVAILABLE:
            return None
        
        try:
            dominant_emotion = max(emotions.items(), key=lambda x: x[1])[0] if emotions else "neutral"
            emotion_intensity = emotions.get(dominant_emotion, 0.5)
            
            # Map emotions to Hume voice characteristics
            voice_config = {
                "anxious": {
                    "voice_name": "calm_therapist",
                    "speed": 0.9,
                    "pitch": 0.95,
                    "emotion_target": "calm"
                },
                "sad": {
                    "voice_name": "warm_supporter", 
                    "speed": 0.85,
                    "pitch": 0.9,
                    "emotion_target": "compassionate"
                },
                "angry": {
                    "voice_name": "patient_guide",
                    "speed": 0.95,
                    "pitch": 0.9,
                    "emotion_target": "understanding"
                },
                "happy": {
                    "voice_name": "energetic_friend",
                    "speed": 1.1,
                    "pitch": 1.05,
                    "emotion_target": "joyful"
                },
                "neutral": {
                    "voice_name": "professional_caring",
                    "speed": 1.0,
                    "pitch": 1.0,
                    "emotion_target": "supportive"
                }
            }
            
            config = voice_config.get(dominant_emotion, voice_config["neutral"])
            
            # Hume TTS API call
            payload = {
                "text": text,
                "voice": {
                    "provider": "HUME_AI",
                    "name": config["voice_name"]
                },
                "prosody": {
                    "emotion": config["emotion_target"],
                    "intensity": min(emotion_intensity, 0.8),
                    "speed": config["speed"],
                    "pitch": config["pitch"]
                },
                "format": "wav"
            }
            
            headers = {
                "X-HUME-API-KEY": HUME_API_KEY,
                "Content-Type": "application/json"
            }
            
            response = requests.post(HUME_TTS_URL, json=payload, headers=headers, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                # Return base64 encoded audio for Make.com
                return result.get('audio_data')
            else:
                print(f"Hume TTS error: {response.status_code} - {response.text}")
                return None
                
        except Exception as e:
            print(f"Hume voice generation error: {e}")
            return None

# Initialize components
ora_emotion = ORAEmotionDetection()
hume_voice = HumeVoiceGenerator()

@app.route("/")
def index():
    """Main page"""
    try:
        return render_template("index.html")
    except:
        return jsonify({
            "message": "ORA + Hume Voice Integration", 
            "status": "operational",
            "ora_emotion_detection": True,
            "hume_voice_available": HUME_AVAILABLE,
            "voice_to_voice_ready": True
        })

@app.route("/health")
def health_check():
    """Health check"""
    webhook_status = {name: bool(url) for name, url in MAKE_WEBHOOKS.items()}
    
    return jsonify({
        "status": "healthy",
        "service": "ora_emotion_hume_voice",
        "ora_emotion_detection": True,
        "hume_voice_available": HUME_AVAILABLE,
        "voice_to_voice_ready": True,
        "make_webhooks_configured": webhook_status,
        "total_actions_taken": sum(len(actions) for actions in action_history.values()),
        "empathic_mode": "ora_emotion_hume_voice",
        "conversation_capable": True
    })

# MAIN ENDPOINT: ORA emotion detection + Hume voice generation
@app.route("/hume_voice", methods=["POST"])
def ora_hume_voice():
    """
    MAIN ENDPOINT: ORA emotion detection + Hume voice generation
    Perfect for Make.com integration
    """
    
    start_time = time.time()
    
    try:
        # Handle both JSON and form data
        if request.is_json:
            data = request.get_json()
            message = data.get('message', '')
            user_id = data.get('user_id', 'make_user')
        else:
            message = request.form.get('message', '')
            user_id = request.form.get('user_id', 'make_user')
        
        if not message:
            return jsonify({
                'success': False,
                'error': 'No message provided',
                'audio_response': None
            }), 400
        
        # Step 1: ORA emotion detection (your existing logic)
        emotions = ora_emotion.detect_emotion_from_text(message)
        dominant_emotion = max(emotions.items(), key=lambda x: x[1])[0] if emotions else "neutral"
        
        # Step 2: ORA empathic response generation (your existing logic)
        assistant_response = ora_emotion.generate_empathic_response(message, emotions, user_id)
        
        # Step 3: Hume voice generation (NEW)
        hume_audio = hume_voice.generate_hume_voice(assistant_response, emotions, user_id)
        
        # Step 4: ORA proactive actions (your existing logic)
        context = EmpathicAgent.analyze_emotional_context(message, dominant_emotion, user_id)
        actions_taken = EmpathicAgent.execute_proactive_actions(context)
        
        processing_time = time.time() - start_time
        
        # Return response for Make.com
        return jsonify({
            'success': True,
            'transcript': message,
            'assistant_response': assistant_response,
            'audio_response': hume_audio,  # Base64 encoded Hume voice
            'emotions': emotions,
            'dominant_emotion': dominant_emotion,
            'actions_taken': actions_taken,
            'urgency': context['urgency'],
            'processing_time': processing_time,
            'ora_emotion_detection': True,
            'hume_voice_generated': bool(hume_audio),
            'voice_to_voice': True,
            'emotional_intelligence': True,
            'conversation_capable': True
        })
        
    except Exception as e:
        processing_time = time.time() - start_time
        return jsonify({
            'success': False,
            'error': str(e),
            'processing_time': processing_time
        }), 500

# Text-only endpoint for testing
@app.route("/text_only", methods=["POST"])
def text_only():
    """Text-only version without voice generation"""
    
    try:
        data = request.get_json()
        message = data.get('message', '')
        user_id = data.get('user_id', 'test_user')
        
        if not message:
            return jsonify({
                'success': False,
                'error': 'No message provided'
            }), 400
        
        # ORA processing without voice
        emotions = ora_emotion.detect_emotion_from_text(message)
        dominant_emotion = max(emotions.items(), key=lambda x: x[1])[0] if emotions else "neutral"
        assistant_response = ora_emotion.generate_empathic_response(message, emotions, user_id)
        
        context = EmpathicAgent.analyze_emotional_context(message, dominant_emotion, user_id)
        actions_taken = EmpathicAgent.execute_proactive_actions(context)
        
        return jsonify({
            'success': True,
            'transcript': message,
            'assistant_response': assistant_response,
            'emotions': emotions,
            'dominant_emotion': dominant_emotion,
            'actions_taken': actions_taken,
            'urgency': context['urgency'],
            'ora_emotion_detection': True,
            'text_mode': True
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

# Your existing routes (kept unchanged)
@app.route("/api/agent/status/<user_id>", methods=["GET"])
def agent_status(user_id):
    user_actions = action_history.get(user_id, [])
    return jsonify({
        "user_id": user_id,
        "total_actions": len(user_actions),
        "recent_actions": user_actions[-5:],
        "agent_active": True,
        "ora_emotion_detection": True,
        "hume_voice_integrated": True,
        "conversation_capable": True,
        "emotional_intelligence": "advanced"
    })

# Test endpoint
@app.route("/test", methods=["GET", "POST"])
def test_integration():
    """Test the ORA + Hume integration"""
    
    if request.method == 'GET':
        return jsonify({
            'status': 'ORA Emotion + Hume Voice Integration Active!',
            'endpoints': ['/hume_voice', '/text_only', '/health'],
            'ora_emotion_detection': True,
            'hume_voice_available': HUME_AVAILABLE,
            'voice_to_voice_ready': True,
            'conversation_capable': True,
            'emotional_intelligence': 'ora_emotion_hume_voice',
            'can_handle': ['voice_conversation', 'emotional_support', 'general_questions'],
            'deployment_status': 'successful'
        })
    
    # Test with sample input
    test_data = request.json or {
        'message': 'Hello, how are you?',
        'user_id': 'test_user'
    }
    
    return ora_hume_voice()

if __name__ == "__main__":
    print("üöÄ Starting ORA + HUME VOICE INTEGRATION")
    print("üß† Emotion Detection: ORA (your existing logic)")
    print(f"üéôÔ∏è Voice Generation: Hume {'Configured' if HUME_AVAILABLE else 'Add HUME_API_KEY'}")
    print("‚ù§Ô∏è Empathy: ORA emotion detection + Hume empathic voice")
    print("üéØ Agent: ORA proactive action execution")
    print("üîó Make.com: Voice workflow ready")
    print("‚ö° Speed: Fast processing with ORA + Hume")
    print("üí¨ Conversation: Handles any conversation")
    print("üìä Webhooks configured:", sum(1 for url in MAKE_WEBHOOKS.values() if url))
    print("üåê MAIN ENDPOINT: /hume_voice")
    print("üó£Ô∏è Voice-to-Voice: ORA emotion + Hume voice")
    print("üé≠ Best of Both: ORA intelligence + Hume voice quality")
    print("‚úÖ Deployment: Ready for Make.com integration")
    
    if not HUME_AVAILABLE:
        print("\n‚ö†Ô∏è TO ENABLE HUME VOICE:")
        print("1. Get API key from: https://platform.hume.ai")
        print("2. Set HUME_API_KEY environment variable in Render")
        print("3. Restart this app")
        print("4. Replace Murf AI with audio playback in Make.com")
    
    port = int(os.environ.get('PORT', 5000))
    app.run(debug=False, host='0.0.0.0', port=port)

