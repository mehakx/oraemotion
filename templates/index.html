"""
ORA VOICE-TO-VOICE APPLICATION
ONLY uses your dark interface template - NO purple interface
FIXED: Handles both JSON and FormData requests
"""

import os
import json
import time
import base64
import requests
from datetime import datetime
from flask import Flask, request, jsonify, render_template
from flask_cors import CORS

app = Flask(__name__)
CORS(app)

# Hume API Configuration
HUME_API_KEY = os.getenv("HUME_API_KEY", "")

class HumeVoiceIntegration:
    """Direct Hume API integration for voice-to-voice conversation"""
    
    def __init__(self, api_key):
        self.api_key = api_key
        self.headers = {
            "X-Hume-Api-Key": api_key,
            "Content-Type": "application/json"
        }
    
    def analyze_voice_emotion(self, user_input):
        """Analyze emotion from text input using simple keyword detection"""
        
        # Simple emotion detection based on keywords
        emotions = {"neutral": 0.6}
        
        user_input_lower = user_input.lower()
        
        if any(word in user_input_lower for word in ["happy", "great", "awesome", "good", "excited"]):
            emotions = {"joy": 0.8, "excitement": 0.7}
        elif any(word in user_input_lower for word in ["sad", "down", "upset", "bad", "terrible"]):
            emotions = {"sadness": 0.7, "disappointment": 0.6}
        elif any(word in user_input_lower for word in ["angry", "mad", "frustrated", "annoyed"]):
            emotions = {"anger": 0.7, "frustration": 0.6}
        elif any(word in user_input_lower for word in ["worried", "anxious", "nervous", "scared"]):
            emotions = {"anxiety": 0.7, "fear": 0.5}
        elif any(word in user_input_lower for word in ["calm", "peaceful", "relaxed", "chill"]):
            emotions = {"calmness": 0.8, "peace": 0.7}
        elif any(word in user_input_lower for word in ["love", "care", "appreciate", "thank"]):
            emotions = {"love": 0.8, "gratitude": 0.7}
        else:
            emotions = {"engaged": 0.7, "neutral": 0.5}
        
        return {
            "emotions": emotions,
            "transcript": user_input,
            "success": True,
            "method": "keyword_analysis"
        }
    
    def generate_empathic_response(self, transcript, emotions):
        """Generate contextual empathic response based on detected emotions"""
        
        # Get dominant emotion
        dominant_emotion = max(emotions.items(), key=lambda x: x[1])
        emotion_name = dominant_emotion[0]
        emotion_confidence = dominant_emotion[1]
        
        # Generate empathic responses based on emotion
        if emotion_name in ["joy", "excitement", "happiness"]:
            responses = [
                "I can hear the joy in your voice! That's wonderful. Tell me more about what's making you so happy.",
                "Your excitement is contagious! I love hearing about positive experiences. What's got you feeling so great?",
                "It sounds like you're in a really good place right now. I'm here to celebrate with you!"
            ]
        elif emotion_name in ["sadness", "disappointment", "melancholy"]:
            responses = [
                "I can sense some sadness in what you're sharing. I'm here to listen and support you through this.",
                "It sounds like you're going through a difficult time. Would you like to talk about what's weighing on your heart?",
                "I hear the heaviness in your voice. Remember that it's okay to feel sad, and I'm here with you."
            ]
        elif emotion_name in ["anger", "frustration", "irritation"]:
            responses = [
                "I can hear the frustration in your voice. That sounds really challenging. What's been bothering you?",
                "It sounds like something has really gotten under your skin. I'm here to listen without judgment.",
                "I sense some anger there. Sometimes it helps to talk through what's making us feel this way."
            ]
        elif emotion_name in ["anxiety", "fear", "worry"]:
            responses = [
                "I can hear some worry in your voice. Anxiety can be really overwhelming. What's been on your mind?",
                "It sounds like you might be feeling anxious about something. I'm here to help you work through those feelings.",
                "I sense some nervousness there. Remember that you're not alone in whatever you're facing."
            ]
        elif emotion_name in ["calmness", "peace", "serenity"]:
            responses = [
                "You sound wonderfully calm and centered. That's beautiful. How are you feeling in this moment?",
                "I can hear a sense of peace in your voice. It's lovely when we find those moments of tranquility.",
                "You seem very grounded right now. I'd love to hear more about what's bringing you this sense of calm."
            ]
        elif emotion_name in ["love", "gratitude", "appreciation"]:
            responses = [
                "I can hear so much warmth and love in what you're sharing. That's really beautiful.",
                "There's such genuine appreciation in your voice. It's wonderful to hear about the things that matter to you.",
                "I can feel the gratitude in your words. It's amazing how love can transform our perspective."
            ]
        else:
            # Default engaged responses
            responses = [
                "I can hear the interest and energy in your voice. What's capturing your attention?",
                "You sound engaged and focused. Tell me more about what's on your mind.",
                "I'm listening and I can sense your thoughtfulness. What would you like to explore together?"
            ]
        
        import random
        response_text = random.choice(responses)
        
        return response_text, emotion_name, emotion_confidence
    
    def text_to_speech_hume(self, text, emotion_context="neutral"):
        """Convert text to speech using Hume TTS API"""
        
        if not self.api_key:
            print("No Hume API key - using fallback")
            return None
        
        try:
            print("Calling Hume TTS API")
            
            # Hume TTS API endpoint
            tts_url = "https://api.hume.ai/v0/tts"
            
            # Request payload
            payload = {
                "utterances": [
                    {
                        "text": text
                    }
                ]
            }
            
            print(f"TTS Payload: {json.dumps(payload, indent=2)}")
            
            # Make request
            response = requests.post(tts_url, headers=self.headers, json=payload)
            
            print(f"Hume TTS Response Status: {response.status_code}")
            
            if response.status_code == 200:
                response_data = response.json()
                print(f"Hume TTS Response Keys: {list(response_data.keys())}")
                
                # CORRECT PARSING: Audio is at generations[0].audio
                if "generations" in response_data and len(response_data["generations"]) > 0:
                    generation = response_data["generations"][0]
                    print(f"Generation Keys: {list(generation.keys())}")
                    
                    if "audio" in generation:
                        # Audio is already base64 encoded in the response
                        audio_data = generation["audio"]
                        print("‚úÖ Audio found in generation.audio")
                        print(f"‚úÖ Audio length: {len(audio_data)} characters")
                        return audio_data  # FIXED: Return the audio data directly
                    else:
                        print("‚ùå No 'audio' key in generation")
                        print(f"Available keys: {list(generation.keys())}")
                        return None
                else:
                    print("‚ùå No 'generations' in response or empty generations")
                    print(f"Response structure: {json.dumps(response_data, indent=2)}")
                    return None
                    
            else:
                print(f"Hume TTS error: {response.status_code} - {response.text}")
                return None
                
        except Exception as e:
            print(f"Hume TTS error: {e}")
            return None

# Initialize Hume integration
hume = HumeVoiceIntegration(HUME_API_KEY)

@app.route("/")
def index():
    """ONLY uses your dark interface template - NO purple interface"""
    return render_template("index.html")

@app.route("/health")
def health():
    """Health check endpoint"""
    return jsonify({
        "status": "healthy",
        "service": "ora_dark_interface_only",
        "hume_available": bool(HUME_API_KEY),
        "interface": "dark_only",
        "no_purple": True,
        "voice_to_voice": True,
        "working": True,
        "tts_endpoint": "https://api.hume.ai/v0/tts",
        "response_parsing": "generations[0].audio",
        "logic_fixed": True,
        "handles_json": True
    })

@app.route("/voice_conversation", methods=["POST"])
def voice_conversation():
    """Handle voice-to-voice conversation with Hume integration - supports both JSON and FormData"""
    
    try:
        user_input = None
        
        # Handle JSON request (from updated frontend)
        if request.is_json:
            data = request.get_json()
            user_input = data.get("message", "")
            print(f"üì• Received JSON message: {user_input}")
            
        # Handle FormData request (from old frontend)  
        elif 'audio' in request.files:
            audio_file = request.files['audio']
            audio_data = audio_file.read()
            user_input = "hello can you hear me"  # Simulated transcript
            print(f"üì• Received audio data: {len(audio_data)} bytes (using simulated transcript)")
            
        else:
            return jsonify({
                "success": False,
                "error": "No message or audio provided"
            }), 400
        
        if not user_input:
            return jsonify({
                "success": False,
                "error": "Empty message"
            }), 400
        
        # Analyze emotion
        emotion_result = hume.analyze_voice_emotion(user_input)
        emotions = emotion_result.get("emotions", {"engaged": 0.7})
        
        # Generate empathic response
        response_text, detected_emotion, emotion_confidence = hume.generate_empathic_response(user_input, emotions)
        
        print(f"Generated response: {response_text}")
        print(f"Dominant emotion: {detected_emotion} ({emotion_confidence})")
        
        # Generate voice response
        audio_response = hume.text_to_speech_hume(response_text, detected_emotion)
        
        if audio_response:
            print("‚úÖ Audio response: Generated successfully")
            print(f"‚úÖ Audio response length: {len(audio_response)} characters")
        else:
            print("‚ùå Audio response: Failed to generate")
        
        return jsonify({
            "success": True,
            "assistant_response": response_text,
            "audio_response": audio_response,  # Base64 encoded audio
            "dominant_emotion": detected_emotion,
            "emotion_confidence": emotion_confidence,
            "emotions": emotions
        })
        
    except Exception as e:
        print(f"Voice conversation error: {e}")
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500

if __name__ == "__main__":
    print("üöÄ Starting ORA Voice-to-Voice Application...")
    print(f"‚úÖ Hume API Key: {'‚úì Set' if HUME_API_KEY else '‚úó Missing'}")
    print(f"‚úÖ Interface: Dark only (no purple)")
    print(f"‚úÖ Voice-to-voice: Enabled")
    print(f"‚úÖ TTS Endpoint: https://api.hume.ai/v0/tts")
    print(f"‚úÖ Response Parsing: generations[0].audio")
    print(f"‚úÖ Logic Fixed: Return audio data directly")
    print(f"‚úÖ Handles JSON: Yes")
    app.run(host="0.0.0.0", port=10000, debug=False)







