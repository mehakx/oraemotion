<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ORA - Voice AI</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            background: #000;
            color: #fff;
            text-align: center;
            padding: 50px;
        }
        
        .container {
            max-width: 600px;
            margin: 0 auto;
        }
        
        h1 {
            color: #ffd700;
            margin-bottom: 30px;
        }
        
        .status {
            font-size: 18px;
            margin: 20px 0;
            padding: 15px;
            background: rgba(255,255,255,0.1);
            border-radius: 10px;
        }
        
        button {
            background: #ffd700;
            color: #000;
            border: none;
            padding: 15px 30px;
            font-size: 16px;
            border-radius: 25px;
            cursor: pointer;
            margin: 10px;
        }
        
        button:hover {
            background: #ffed4e;
        }
        
        button:disabled {
            background: #666;
            cursor: not-allowed;
        }
        
        .personality-buttons {
            margin: 20px 0;
        }
        
        .personality-btn {
            background: rgba(255,255,255,0.2);
            color: #fff;
            padding: 10px 20px;
            margin: 5px;
        }
        
        .personality-btn.active {
            background: #ffd700;
            color: #000;
        }
        
        .transcript {
            background: rgba(255,255,255,0.1);
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
            min-height: 100px;
            text-align: left;
            max-height: 300px;
            overflow-y: auto;
        }

        .sun {
            width: 200px;
            height: 200px;
            border-radius: 50%;
            margin: 20px auto;
            background: radial-gradient(circle at 30% 30%, 
                rgba(255, 215, 0, 0.9) 0%,
                rgba(255, 193, 7, 0.8) 30%,
                rgba(255, 152, 0, 0.7) 60%,
                rgba(255, 87, 34, 0.6) 100%);
            box-shadow: 
                0 0 60px rgba(255, 215, 0, 0.4),
                0 0 120px rgba(255, 215, 0, 0.2);
            transition: all 0.3s ease;
        }

        .sun.listening {
            background: radial-gradient(circle at 30% 30%, 
                rgba(33, 150, 243, 0.9) 0%,
                rgba(30, 136, 229, 0.8) 30%,
                rgba(25, 118, 210, 0.7) 60%,
                rgba(21, 101, 192, 0.6) 100%);
            box-shadow: 
                0 0 60px rgba(33, 150, 243, 0.4),
                0 0 120px rgba(33, 150, 243, 0.2);
            animation: pulse 1.5s ease-in-out infinite;
        }

        .sun.speaking {
            background: radial-gradient(circle at 30% 30%, 
                rgba(76, 175, 80, 0.9) 0%,
                rgba(67, 160, 71, 0.8) 30%,
                rgba(56, 142, 60, 0.7) 60%,
                rgba(46, 125, 50, 0.6) 100%);
            box-shadow: 
                0 0 60px rgba(76, 175, 80, 0.4),
                0 0 120px rgba(76, 175, 80, 0.2);
            animation: pulse 0.8s ease-in-out infinite;
        }

        @keyframes pulse {
            0%, 100% { transform: scale(1); }
            50% { transform: scale(1.05); }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>ORA - Voice AI</h1>
        
        <div class="sun" id="sun"></div>
        
        <div class="status" id="status">Initializing...</div>
        
        <div class="personality-buttons">
            <button class="personality-btn active" onclick="setPersonality('empathetic')">Empathetic Friend</button>
            <button class="personality-btn" onclick="setPersonality('practical')">Practical Coach</button>
            <button class="personality-btn" onclick="setPersonality('wise')">Wise Mentor</button>
        </div>
        
        <button id="startBtn" onclick="startConversation()" disabled>Start Conversation</button>
        <button id="stopBtn" onclick="stopConversation()" disabled>Stop Conversation</button>
        
        <div class="transcript" id="transcript">Conversation will appear here...</div>
    </div>

    <script>
        let socket = null;
        let recorder = null;
        let isConnected = false;
        let currentPersonality = 'empathetic';
        let apiKey = null;
        let audioContext = null;
        let currentAudioElement = null;
        let isPlayingAudio = false;
        let audioChunks = [];
        let isCollectingAudio = false;
        let audioTimeout = null;

        const personalities = {
            empathetic: {
                prompt: "You are a warm, empathetic AI friend named 'The Empathetic Friend'. Listen actively, validate emotions, and provide emotional support. Be understanding, compassionate, and caring. Keep responses conversational and under 2 sentences.",
                name: "The Empathetic Friend"
            },
            practical: {
                prompt: "You are a practical, solution-focused AI coach named 'The Practical Coach'. Provide direct, actionable advice and help users solve problems efficiently. Be encouraging but concise and results-oriented. Keep responses brief and under 2 sentences.",
                name: "The Practical Coach"
            },
            wise: {
                prompt: "You are a wise, thoughtful AI mentor named 'The Wise Mentor'. Guide users through reflection and deeper thinking. Ask meaningful questions and share insights. Be thoughtful and philosophical but keep responses under 2 sentences.",
                name: "The Wise Mentor"
            }
        };

        // Initialize
        window.onload = function() {
            console.log('üöÄ Initializing...');
            initializeAudioContext();
            getApiKey();
        };

        function initializeAudioContext() {
            try {
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                console.log('üîä Audio context initialized');
            } catch (error) {
                console.error('‚ùå Audio context error:', error);
            }
        }

        function getApiKey() {
            fetch('/api/hume-key')
                .then(response => response.json())
                .then(data => {
                    if (data.api_key) {
                        apiKey = data.api_key;
                        updateStatus('Ready to start conversation');
                        document.getElementById('startBtn').disabled = false;
                        console.log('‚úÖ API key loaded successfully');
                    } else {
                        throw new Error('No API key received');
                    }
                })
                .catch(error => {
                    console.error('‚ùå Error getting API key:', error);
                    updateStatus('Error: Could not get API key');
                });
        }

        function setPersonality(personality) {
            // Stop any current audio when switching personalities
            stopCurrentAudio();
            
            currentPersonality = personality;
            
            // Update button styles
            document.querySelectorAll('.personality-btn').forEach(btn => {
                btn.classList.remove('active');
            });
            event.target.classList.add('active');
            
            const personalityInfo = personalities[personality];
            updateStatus(`Personality set to: ${personalityInfo.name}`);
            console.log('üé≠ Personality changed to:', personalityInfo.name);
            
            // If conversation is active, send new personality prompt
            if (isConnected && socket && socket.readyState === WebSocket.OPEN) {
                console.log('üîÑ Updating personality during active conversation');
                const systemMessage = {
                    type: 'session_settings',
                    system_prompt: personalityInfo.prompt + " The user's name is User. You just switched personalities - acknowledge this change briefly and continue the conversation in your new personality style.",
                    language: 'en'
                };
                socket.send(JSON.stringify(systemMessage));
                
                addToTranscript(`System: Switched to ${personalityInfo.name}`);
                updateStatus(`Now speaking as: ${personalityInfo.name}`);
            }
        }

        function startConversation() {
            if (!apiKey) {
                updateStatus('Error: No API key available');
                return;
            }

            updateStatus('Connecting to Hume EVI...');
            document.getElementById('startBtn').disabled = true;

            try {
                // Connect to Hume EVI WebSocket
                const wsUrl = `wss://api.hume.ai/v0/evi/chat?api_key=${encodeURIComponent(apiKey)}`;
                socket = new WebSocket(wsUrl);

                socket.onopen = function() {
                    console.log('‚úÖ Connected to Hume EVI');
                    updateStatus('Connected! Starting microphone...');
                    
                    // Send initial personality prompt
                    const personalityInfo = personalities[currentPersonality];
                    const systemMessage = {
                        type: 'session_settings',
                        system_prompt: personalityInfo.prompt + " The user's name is User. Greet them warmly and introduce yourself as " + personalityInfo.name + ".",
                        language: 'en'
                    };
                    socket.send(JSON.stringify(systemMessage));
                    
                    console.log('üé≠ Set personality:', personalityInfo.name);
                    
                    startMicrophone();
                };

                socket.onmessage = function(event) {
                    const data = JSON.parse(event.data);
                    console.log('üì® Received:', data.type);
                    
                    if (data.type === 'user_message') {
                        const content = data.message?.content || '[Speaking...]';
                        addToTranscript('You: ' + content);
                        updateStatus('Processing your message...');
                        updateSunState('processing');
                    } else if (data.type === 'assistant_message') {
                        const content = data.message?.content || '[Responding...]';
                        addToTranscript('AI: ' + content);
                        updateStatus('AI is speaking...');
                        updateSunState('speaking');
                        
                        // Start collecting audio chunks for this response
                        audioChunks = [];
                        isCollectingAudio = true;
                        console.log('üéµ Started collecting audio chunks for response');
                        
                        // Set timeout to play audio if assistant_end doesn't come
                        if (audioTimeout) clearTimeout(audioTimeout);
                        audioTimeout = setTimeout(() => {
                            console.log('‚è∞ Timeout - playing collected audio chunks');
                            playCollectedAudio();
                        }, 3000); // 3 second timeout
                        
                    } else if (data.type === 'audio_output') {
                        console.log('üîä Received audio chunk');
                        
                        if (isCollectingAudio) {
                            // Collect this audio chunk
                            audioChunks.push(data.data);
                            console.log('üì¶ Collected audio chunk', audioChunks.length);
                            
                            // If we have enough chunks, play immediately
                            if (audioChunks.length >= 3) {
                                console.log('üéµ Have enough chunks - playing now');
                                playCollectedAudio();
                            }
                        } else {
                            // Play individual chunk immediately if not collecting
                            console.log('üîä Playing individual audio chunk');
                            playAudioChunk(data.data);
                        }
                        
                    } else if (data.type === 'assistant_end') {
                        console.log('üéµ Assistant finished - playing collected audio');
                        playCollectedAudio();
                        
                    } else if (data.type === 'user_interruption') {
                        // User interrupted - stop collecting and playing audio
                        isCollectingAudio = false;
                        audioChunks = [];
                        if (audioTimeout) clearTimeout(audioTimeout);
                        stopCurrentAudio();
                        updateStatus('Listening... (continue speaking)');
                        updateSunState('listening');
                    }
                };

                socket.onerror = function(error) {
                    console.error('‚ùå WebSocket error:', error);
                    updateStatus('Connection error');
                    document.getElementById('startBtn').disabled = false;
                };

                socket.onclose = function() {
                    console.log('üîå WebSocket closed');
                    updateStatus('Disconnected');
                    document.getElementById('startBtn').disabled = false;
                    document.getElementById('stopBtn').disabled = true;
                    updateSunState('idle');
                    isConnected = false;
                };

            } catch (error) {
                console.error('‚ùå Error starting conversation:', error);
                updateStatus('Error starting conversation');
                document.getElementById('startBtn').disabled = false;
            }
        }

        function startMicrophone() {
            navigator.mediaDevices.getUserMedia({ 
                audio: {
                    echoCancellation: true,
                    noiseSuppression: true,
                    autoGainControl: true
                }
            })
                .then(function(stream) {
                    console.log('üéôÔ∏è Microphone access granted');
                    
                    recorder = new MediaRecorder(stream, {
                        mimeType: 'audio/webm;codecs=opus'
                    });

                    recorder.ondataavailable = function(event) {
                        if (event.data.size > 0 && socket && socket.readyState === WebSocket.OPEN) {
                            const reader = new FileReader();
                            reader.onloadend = function() {
                                const base64Audio = reader.result.split(',')[1];
                                socket.send(JSON.stringify({
                                    type: 'audio_input',
                                    data: base64Audio
                                }));
                            };
                            reader.readAsDataURL(event.data);
                        }
                    };

                    recorder.start(100); // Send audio every 100ms
                    
                    isConnected = true;
                    updateStatus('Listening... (speak now)');
                    updateSunState('listening');
                    document.getElementById('stopBtn').disabled = false;
                    
                    console.log('üé§ Recording started');
                })
                .catch(function(error) {
                    console.error('‚ùå Microphone error:', error);
                    updateStatus('Microphone access denied');
                    if (socket) socket.close();
                });
        }

        function playCollectedAudio() {
            if (audioChunks.length === 0) {
                console.log('‚ö†Ô∏è No audio chunks to play');
                updateStatus('Listening... (speak now)');
                updateSunState('listening');
                return;
            }
            
            console.log('üéµ Playing collected audio from', audioChunks.length, 'chunks');
            
            // Stop collecting
            isCollectingAudio = false;
            if (audioTimeout) clearTimeout(audioTimeout);
            
            // Stop any current audio first
            stopCurrentAudio();
            
            try {
                // Use the first chunk for now (simplest approach)
                const audioData = audioChunks[0];
                playAudioChunk(audioData);
                
                // Clear chunks
                audioChunks = [];
                
            } catch (error) {
                console.error('‚ùå Error playing collected audio:', error);
                updateStatus('Listening... (speak now)');
                updateSunState('listening');
            }
        }

        function playAudioChunk(audioData) {
            try {
                console.log('üîä Playing audio chunk...');
                
                // Decode base64 audio
                const binaryString = atob(audioData);
                const bytes = new Uint8Array(binaryString.length);
                for (let i = 0; i < binaryString.length; i++) {
                    bytes[i] = binaryString.charCodeAt(i);
                }
                
                const audioBlob = new Blob([bytes], { type: 'audio/wav' });
                const audioUrl = URL.createObjectURL(audioBlob);
                
                currentAudioElement = new Audio();
                
                currentAudioElement.oncanplaythrough = function() {
                    console.log('üîä Audio ready to play');
                    currentAudioElement.play().catch(error => {
                        console.error('‚ùå Audio play error:', error);
                        isPlayingAudio = false;
                        updateStatus('Listening... (speak now)');
                        updateSunState('listening');
                    });
                };
                
                currentAudioElement.onplay = function() {
                    console.log('üîä Audio playback started');
                    isPlayingAudio = true;
                };
                
                currentAudioElement.onended = function() {
                    console.log('üîä Audio playback ended');
                    URL.revokeObjectURL(audioUrl);
                    currentAudioElement = null;
                    isPlayingAudio = false;
                    updateStatus('Listening... (speak now)');
                    updateSunState('listening');
                };
                
                currentAudioElement.onerror = function(error) {
                    console.error('‚ùå Audio error:', error);
                    URL.revokeObjectURL(audioUrl);
                    currentAudioElement = null;
                    isPlayingAudio = false;
                    updateStatus('Listening... (speak now)');
                    updateSunState('listening');
                };
                
                currentAudioElement.src = audioUrl;
                currentAudioElement.load();
                
            } catch (error) {
                console.error('‚ùå Error playing audio chunk:', error);
                updateStatus('Listening... (speak now)');
                updateSunState('listening');
            }
        }

        function stopCurrentAudio() {
            // Stop HTML Audio element
            if (currentAudioElement) {
                currentAudioElement.pause();
                currentAudioElement.currentTime = 0;
                currentAudioElement = null;
            }
            
            // Stop all other audio elements
            const audioElements = document.querySelectorAll('audio');
            audioElements.forEach(audio => {
                audio.pause();
                audio.currentTime = 0;
            });
            
            isPlayingAudio = false;
            console.log('üîá All audio stopped');
        }

        function stopConversation() {
            console.log('üîö Stopping conversation');
            
            if (recorder) {
                recorder.stop();
                recorder.stream.getTracks().forEach(track => track.stop());
                recorder = null;
            }
            
            stopCurrentAudio();
            isCollectingAudio = false;
            audioChunks = [];
            if (audioTimeout) clearTimeout(audioTimeout);
            
            if (socket) {
                socket.close();
                socket = null;
            }
            
            updateStatus('Conversation stopped');
            updateSunState('idle');
            document.getElementById('startBtn').disabled = false;
            document.getElementById('stopBtn').disabled = true;
            isConnected = false;
        }

        function updateStatus(message) {
            document.getElementById('status').textContent = message;
            console.log('üìä Status:', message);
        }

        function updateSunState(state) {
            const sun = document.getElementById('sun');
            sun.className = 'sun';
            if (state !== 'idle') {
                sun.classList.add(state);
            }
        }

        function addToTranscript(message) {
            const transcript = document.getElementById('transcript');
            const timestamp = new Date().toLocaleTimeString();
            transcript.innerHTML += `<div><small>${timestamp}</small><br>${message}</div><br>`;
            transcript.scrollTop = transcript.scrollHeight;
        }

        // Handle page unload
        window.addEventListener('beforeunload', function() {
            if (isConnected) {
                stopConversation();
            }
        });

        // Handle user interaction for audio context
        document.addEventListener('click', function() {
            if (audioContext && audioContext.state === 'suspended') {
                audioContext.resume();
                console.log('üîä Audio context resumed');
            }
        });
    </script>
</body>
</html>



