<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ORA - Voice AI</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            background: #000;
            color: #fff;
            text-align: center;
            padding: 50px;
        }
        
        .container {
            max-width: 800px;
            margin: 0 auto;
        }
        
        h1 {
            color: #ffd700;
            margin-bottom: 30px;
        }
        
        .status {
            font-size: 18px;
            margin: 20px 0;
            padding: 15px;
            background: rgba(255,255,255,0.1);
            border-radius: 10px;
        }
        
        .personality-cards {
            display: flex;
            gap: 20px;
            justify-content: center;
            margin: 30px 0;
            flex-wrap: wrap;
        }
        
        .personality-card {
            background: rgba(255,255,255,0.1);
            border-radius: 15px;
            padding: 20px;
            width: 220px;
            text-align: center;
            transition: all 0.3s ease;
            border: 2px solid transparent;
        }
        
        .personality-card.active {
            border: 2px solid #ffd700;
            background: rgba(255,215,0,0.1);
            box-shadow: 0 0 20px rgba(255,215,0,0.3);
        }
        
        .personality-card.inactive {
            opacity: 0.5;
            background: rgba(255,255,255,0.05);
        }
        
        .personality-card h3 {
            color: #ffd700;
            margin-bottom: 15px;
        }
        
        .personality-card .status {
            font-size: 14px;
            margin: 10px 0;
            padding: 8px;
            background: rgba(0,0,0,0.3);
        }
        
        .personality-btn {
            background: #ffd700;
            color: #000;
            border: none;
            padding: 12px 20px;
            font-size: 14px;
            border-radius: 20px;
            cursor: pointer;
            margin: 5px;
            width: 100%;
        }
        
        .personality-btn:hover {
            background: #ffed4e;
        }
        
        .personality-btn:disabled {
            background: #666;
            cursor: not-allowed;
        }
        
        .personality-btn.active {
            background: #4CAF50;
            color: white;
        }
        
        .sun {
            width: 100px;
            height: 100px;
            border-radius: 50%;
            margin: 15px auto;
            background: radial-gradient(circle at 30% 30%, 
                rgba(255, 215, 0, 0.9) 0%,
                rgba(255, 193, 7, 0.8) 30%,
                rgba(255, 152, 0, 0.7) 60%,
                rgba(255, 87, 34, 0.6) 100%);
            box-shadow: 
                0 0 30px rgba(255, 215, 0, 0.4),
                0 0 60px rgba(255, 215, 0, 0.2);
            transition: all 0.3s ease;
        }

        .sun.listening {
            background: radial-gradient(circle at 30% 30%, 
                rgba(33, 150, 243, 0.9) 0%,
                rgba(30, 136, 229, 0.8) 30%,
                rgba(25, 118, 210, 0.7) 60%,
                rgba(21, 101, 192, 0.6) 100%);
            box-shadow: 
                0 0 30px rgba(33, 150, 243, 0.4),
                0 0 60px rgba(33, 150, 243, 0.2);
            animation: pulse 1.5s ease-in-out infinite;
        }

        .sun.speaking {
            background: radial-gradient(circle at 30% 30%, 
                rgba(76, 175, 80, 0.9) 0%,
                rgba(67, 160, 71, 0.8) 30%,
                rgba(56, 142, 60, 0.7) 60%,
                rgba(46, 125, 50, 0.6) 100%);
            box-shadow: 
                0 0 30px rgba(76, 175, 80, 0.4),
                0 0 60px rgba(76, 175, 80, 0.2);
            animation: pulse 0.8s ease-in-out infinite;
        }

        @keyframes pulse {
            0%, 100% { transform: scale(1); }
            50% { transform: scale(1.05); }
        }

        .transcript {
            background: rgba(255,255,255,0.1);
            padding: 15px;
            border-radius: 10px;
            margin: 15px 0;
            min-height: 80px;
            text-align: left;
            max-height: 200px;
            overflow-y: auto;
            font-size: 12px;
        }

        .global-controls {
            margin: 30px 0;
        }

        .global-controls button {
            background: #ff4444;
            color: white;
            border: none;
            padding: 15px 30px;
            font-size: 16px;
            border-radius: 25px;
            cursor: pointer;
            margin: 10px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>ORA - Voice AI</h1>
        
        <div class="status" id="globalStatus">Initializing EVI configurations...</div>
        
        <div class="personality-cards">
            <!-- Empathetic Friend -->
            <div class="personality-card" id="card-empathetic">
                <h3>Empathetic Friend</h3>
                <div class="sun" id="sun-empathetic"></div>
                <div class="status" id="status-empathetic">Initializing...</div>
                <button class="personality-btn" id="btn-empathetic" onclick="activatePersonality('empathetic')" disabled>Loading...</button>
                <div class="transcript" id="transcript-empathetic">Conversation will appear here...</div>
            </div>
            
            <!-- Practical Coach -->
            <div class="personality-card" id="card-practical">
                <h3>Practical Coach</h3>
                <div class="sun" id="sun-practical"></div>
                <div class="status" id="status-practical">Initializing...</div>
                <button class="personality-btn" id="btn-practical" onclick="activatePersonality('practical')" disabled>Loading...</button>
                <div class="transcript" id="transcript-practical">Conversation will appear here...</div>
            </div>
            
            <!-- Wise Mentor -->
            <div class="personality-card" id="card-wise">
                <h3>Wise Mentor</h3>
                <div class="sun" id="sun-wise"></div>
                <div class="status" id="status-wise">Initializing...</div>
                <button class="personality-btn" id="btn-wise" onclick="activatePersonality('wise')" disabled>Loading...</button>
                <div class="transcript" id="transcript-wise">Conversation will appear here...</div>
            </div>
        </div>
        
        <div class="global-controls">
            <button onclick="stopAllConversations()">Stop All Conversations</button>
        </div>
    </div>

    <script>
        let apiKey = null;
        let audioContext = null;
        let sharedMicrophoneStream = null;
        let currentActivePersonality = null;
        
        // Generate unique session ID to avoid config name conflicts
        const sessionId = Date.now().toString(36) + Math.random().toString(36).substr(2);
        
        // Separate state for each personality with their EVI config IDs
        const personalities = {
            empathetic: {
                socket: null,
                recorder: null,
                isConnected: false,
                audioQueue: [],
                isPlayingAudio: false,
                audioBuffer: [],
                bufferTimeout: null,
                currentSource: null,
                nextStartTime: 0,
                voice: 'KORA', // Female voice
                configId: null, // Will be set after creating EVI config
                prompt: "You are a warm, empathetic AI friend named 'The Empathetic Friend'. Listen actively, validate emotions, and provide emotional support. Be understanding, compassionate, and caring. Keep responses conversational and under 2 sentences.",
                name: "The Empathetic Friend",
                responseState: 'listening', // Track response state
                hasPlayedResponse: false // Track if response was already played
            },
            practical: {
                socket: null,
                recorder: null,
                isConnected: false,
                audioQueue: [],
                isPlayingAudio: false,
                audioBuffer: [],
                bufferTimeout: null,
                currentSource: null,
                nextStartTime: 0,
                voice: 'DACHER', // Male voice
                configId: null, // Will be set after creating EVI config
                prompt: "You are a practical, solution-focused AI coach named 'The Practical Coach'. Provide direct, actionable advice and help users solve problems efficiently. Be encouraging but concise and results-oriented. Keep responses brief and under 2 sentences.",
                name: "The Practical Coach",
                responseState: 'listening', // Track response state
                hasPlayedResponse: false // Track if response was already played
            },
            wise: {
                socket: null,
                recorder: null,
                isConnected: false,
                audioQueue: [],
                isPlayingAudio: false,
                audioBuffer: [],
                bufferTimeout: null,
                currentSource: null,
                nextStartTime: 0,
                voice: 'STELLA', // Thoughtful voice
                configId: null, // Will be set after creating EVI config
                prompt: "You are a wise, thoughtful AI mentor named 'The Wise Mentor'. Guide users through reflection and deeper thinking. Ask meaningful questions and share insights. Be thoughtful and philosophical but keep responses under 2 sentences.",
                name: "The Wise Mentor",
                responseState: 'listening', // Track response state
                hasPlayedResponse: false // Track if response was already played
            }
        };

        // Initialize
        window.onload = function() {
            console.log('üöÄ Initializing WebSocket Fixed Version...');
            console.log('üÜî Session ID:', sessionId);
            initializeAudioContext();
            getApiKeyAndCreateConfigs();
        };

        function initializeAudioContext() {
            try {
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                console.log('üîä Web Audio API context initialized');
            } catch (error) {
                console.error('‚ùå Audio context error:', error);
            }
        }

        async function getApiKeyAndCreateConfigs() {
            try {
                // Get API key
                const response = await fetch('/api/hume-key');
                const data = await response.json();
                
                if (!data.api_key) {
                    throw new Error('No API key received');
                }
                
                apiKey = data.api_key;
                console.log('‚úÖ API key loaded successfully');
                
                // Create EVI configurations for each personality
                await createEVIConfigurations();
                
            } catch (error) {
                console.error('‚ùå Error getting API key:', error);
                updateGlobalStatus('Error: Could not get API key');
            }
        }

        async function createEVIConfigurations() {
            updateGlobalStatus('Creating EVI configurations with distinct voices...');
            
            try {
                // Create configurations for each personality
                for (const [personalityType, personality] of Object.entries(personalities)) {
                    console.log(`üîß Creating EVI config for ${personalityType} with voice: ${personality.voice}`);
                    
                    // Use unique name with session ID to avoid conflicts
                    const uniqueName = `${personality.name} Config ${sessionId}`;
                    
                    const configData = {
                        evi_version: "2",
                        name: uniqueName,
                        version_description: `Configuration for ${personality.name} with ${personality.voice} voice (Session: ${sessionId})`,
                        prompt: {
                            text: personality.prompt + " The user's name is User. Greet them warmly and introduce yourself as " + personality.name + ". Then ask them how they're doing today."
                        },
                        voice: {
                            name: personality.voice,
                            provider: "HUME_AI"
                        }
                    };
                    
                    console.log(`üîß ${personalityType} config data:`, JSON.stringify(configData, null, 2));
                    
                    const configResponse = await fetch('https://api.hume.ai/v0/evi/configs', {
                        method: 'POST',
                        headers: {
                            'X-Hume-Api-Key': apiKey,
                            'Content-Type': 'application/json'
                        },
                        body: JSON.stringify(configData)
                    });
                    
                    console.log(`üîß ${personalityType} config response status:`, configResponse.status);
                    
                    if (!configResponse.ok) {
                        const errorText = await configResponse.text();
                        console.error(`‚ùå ${personalityType} config error response:`, errorText);
                        throw new Error(`Failed to create config for ${personalityType}: ${configResponse.status} - ${errorText}`);
                    }
                    
                    const configResult = await configResponse.json();
                    personality.configId = configResult.id;
                    
                    console.log(`‚úÖ Created EVI config for ${personalityType}: ${personality.configId} with voice: ${personality.voice}`);
                    updatePersonalityStatus(personalityType, `Ready (${personality.voice} voice)`);
                    updatePersonalityButton(personalityType, 'Start Conversation', false);
                }
                
                updateGlobalStatus('‚úÖ All personalities ready with distinct voices! Click any to start conversation');
                console.log('üé≠ All EVI configurations created successfully with distinct voices!');
                
            } catch (error) {
                console.error('‚ùå Error creating EVI configurations:', error);
                updateGlobalStatus('Error creating voice configurations: ' + error.message);
                
                // Fallback: use default configuration without custom voices
                console.log('üîÑ Falling back to default configuration...');
                Object.keys(personalities).forEach(type => {
                    personalities[type].configId = null; // Use default
                    updatePersonalityStatus(type, 'Ready (default voice)');
                    updatePersonalityButton(type, 'Start Conversation', false);
                });
                updateGlobalStatus('Ready! Using default voices - click any personality to start conversation');
            }
        }

        async function initializeMicrophone() {
            if (sharedMicrophoneStream) {
                console.log('üéôÔ∏è Using existing microphone stream');
                return sharedMicrophoneStream;
            }

            try {
                sharedMicrophoneStream = await navigator.mediaDevices.getUserMedia({ 
                    audio: {
                        echoCancellation: true,
                        noiseSuppression: true,
                        autoGainControl: true
                    }
                });
                console.log('üéôÔ∏è Shared microphone stream initialized');
                return sharedMicrophoneStream;
            } catch (error) {
                console.error('‚ùå Microphone initialization error:', error);
                throw error;
            }
        }

        function activatePersonality(personalityType) {
            console.log(`üé≠ Activating personality: ${personalityType}`);
            
            // If this personality is already active, deactivate it
            if (currentActivePersonality === personalityType) {
                deactivateCurrentPersonality();
                return;
            }
            
            // Deactivate current personality if any
            if (currentActivePersonality) {
                deactivateCurrentPersonality();
            }
            
            // Activate new personality
            startPersonalityConversation(personalityType);
        }

        function deactivateCurrentPersonality() {
            if (!currentActivePersonality) return;
            
            console.log(`üîá Deactivating current personality: ${currentActivePersonality}`);
            stopPersonalityConversation(currentActivePersonality);
            currentActivePersonality = null;
            updatePersonalityCardStates();
        }

        async function startPersonalityConversation(personalityType) {
            if (!apiKey) {
                updatePersonalityStatus(personalityType, 'Error: No API key');
                return;
            }

            const personality = personalities[personalityType];
            
            updatePersonalityStatus(personalityType, 'Connecting...');
            updatePersonalityButton(personalityType, 'Connecting...', true);

            try {
                // Initialize microphone if needed
                await initializeMicrophone();
                
                // Build WebSocket URL with config_id if available
                let wsUrl = `wss://api.hume.ai/v0/evi/chat?api_key=${encodeURIComponent(apiKey)}`;
                if (personality.configId) {
                    wsUrl += `&config_id=${encodeURIComponent(personality.configId)}`;
                    console.log(`üéµ ${personalityType} using EVI config: ${personality.configId} with voice: ${personality.voice}`);
                } else {
                    console.log(`‚ö†Ô∏è ${personalityType} using default configuration (no custom voice)`);
                }
                
                personality.socket = new WebSocket(wsUrl);

                personality.socket.onopen = function() {
                    console.log(`‚úÖ ${personalityType} connected to Hume EVI with custom voice configuration`);
                    
                    // Start recording immediately - no need for session_settings since voice is in config
                    startPersonalityRecording(personalityType);
                };

                personality.socket.onmessage = function(event) {
                    const data = JSON.parse(event.data);
                    console.log(`üì® ${personalityType} received:`, data.type);
                    
                    if (data.type === 'user_message') {
                        const content = data.message?.content || '[Speaking...]';
                        addToPersonalityTranscript(personalityType, 'You: ' + content);
                        updatePersonalityStatus(personalityType, 'Processing...');
                        updatePersonalitySun(personalityType, 'processing');
                        
                        // Reset response state when user speaks
                        personality.responseState = 'processing';
                        personality.hasPlayedResponse = false;
                        personality.audioBuffer = []; // Clear any old audio
                        
                    } else if (data.type === 'assistant_message') {
                        const content = data.message?.content || '[Responding...]';
                        addToPersonalityTranscript(personalityType, 'AI: ' + content);
                        updatePersonalityStatus(personalityType, 'Speaking...');
                        updatePersonalitySun(personalityType, 'speaking');
                        
                        // Set response state to speaking
                        personality.responseState = 'speaking';
                        
                    } else if (data.type === 'audio_output') {
                        console.log(`üîä ${personalityType} received audio chunk`);
                        // Only play audio if this personality is currently active AND we haven't played this response yet
                        if (currentActivePersonality === personalityType && !personality.hasPlayedResponse) {
                            bufferAudioChunk(personalityType, data.data);
                        } else {
                            console.log(`üîá Ignoring audio from ${personalityType} - already played or inactive`);
                        }
                    } else if (data.type === 'assistant_end') {
                        console.log(`üéµ ${personalityType} assistant finished`);
                        // Only flush audio if we haven't played this response yet
                        if (currentActivePersonality === personalityType && !personality.hasPlayedResponse) {
                            flushAudioBuffer(personalityType);
                        }
                    } else if (data.type === 'user_interruption') {
                        // User interrupted - stop playing audio and reset state
                        if (currentActivePersonality === personalityType) {
                            stopPersonalityAudio(personalityType);
                            personality.responseState = 'listening';
                            personality.hasPlayedResponse = false;
                            updatePersonalityStatus(personalityType, 'Listening...');
                            updatePersonalitySun(personalityType, 'listening');
                        }
                    }
                };

                personality.socket.onerror = function(error) {
                    console.error(`‚ùå ${personalityType} WebSocket error:`, error);
                    updatePersonalityStatus(personalityType, 'Connection error');
                    updatePersonalityButton(personalityType, 'Start Conversation', false);
                };

                personality.socket.onclose = function() {
                    console.log(`üîå ${personalityType} WebSocket closed`);
                    updatePersonalityStatus(personalityType, 'Disconnected');
                    updatePersonalityButton(personalityType, 'Start Conversation', false);
                    updatePersonalitySun(personalityType, '');
                    
                    // Clean up recorder when socket closes
                    if (personality.recorder) {
                        try {
                            personality.recorder.stop();
                        } catch (e) {
                            // Recorder might already be stopped
                        }
                        personality.recorder = null;
                    }
                    
                    personality.isConnected = false;
                    
                    if (currentActivePersonality === personalityType) {
                        currentActivePersonality = null;
                        updatePersonalityCardStates();
                    }
                };

            } catch (error) {
                console.error(`‚ùå Error starting ${personalityType}:`, error);
                updatePersonalityStatus(personalityType, 'Error: ' + error.message);
                updatePersonalityButton(personalityType, 'Start Conversation', false);
            }
        }

        function startPersonalityRecording(personalityType) {
            const personality = personalities[personalityType];
            
            if (!sharedMicrophoneStream) {
                console.error('‚ùå No microphone stream available');
                return;
            }

            try {
                personality.recorder = new MediaRecorder(sharedMicrophoneStream, {
                    mimeType: 'audio/webm;codecs=opus'
                });

                personality.recorder.ondataavailable = function(event) {
                    // NEW: Check WebSocket state before sending
                    if (currentActivePersonality === personalityType && 
                        event.data.size > 0 && 
                        personality.socket && 
                        personality.socket.readyState === WebSocket.OPEN) {
                        
                        const reader = new FileReader();
                        reader.onload = function() {
                            // Double-check WebSocket state again before sending
                            if (personality.socket && personality.socket.readyState === WebSocket.OPEN) {
                                const base64Audio = reader.result.split(',')[1];
                                const audioMessage = {
                                    type: 'audio_input',
                                    data: base64Audio
                                };
                                
                                try {
                                    personality.socket.send(JSON.stringify(audioMessage));
                                } catch (error) {
                                    console.error(`‚ùå ${personalityType} failed to send audio:`, error);
                                }
                            } else {
                                console.log(`üîá ${personalityType} WebSocket not ready, skipping audio chunk`);
                            }
                        };
                        reader.readAsDataURL(event.data);
                    }
                };

                personality.recorder.start(100); // Send audio chunks every 100ms
                personality.isConnected = true;
                currentActivePersonality = personalityType;
                
                // Initialize response state
                personality.responseState = 'listening';
                personality.hasPlayedResponse = false;
                
                updatePersonalityStatus(personalityType, 'Listening...');
                updatePersonalityButton(personalityType, 'Stop Conversation', false);
                updatePersonalitySun(personalityType, 'listening');
                updatePersonalityCardStates();
                updateGlobalStatus(`Active: ${personality.name} (${personality.voice} voice)`);
                
                console.log(`üéôÔ∏è ${personalityType} recording started with ${personality.voice} voice via config: ${personality.configId}`);

            } catch (error) {
                console.error(`‚ùå Error starting recording for ${personalityType}:`, error);
                updatePersonalityStatus(personalityType, 'Recording error');
            }
        }

        function stopPersonalityConversation(personalityType) {
            const personality = personalities[personalityType];
            
            // Stop recorder first
            if (personality.recorder) {
                try {
                    personality.recorder.stop();
                } catch (error) {
                    console.log(`‚ö†Ô∏è ${personalityType} recorder already stopped`);
                }
                personality.recorder = null;
            }
            
            // Close WebSocket
            if (personality.socket) {
                try {
                    personality.socket.close();
                } catch (error) {
                    console.log(`‚ö†Ô∏è ${personalityType} WebSocket already closed`);
                }
                personality.socket = null;
            }
            
            // Stop any playing audio
            stopPersonalityAudio(personalityType);
            
            personality.isConnected = false;
            personality.audioBuffer = [];
            personality.responseState = 'listening';
            personality.hasPlayedResponse = false;
            
            updatePersonalityStatus(personalityType, `Ready (${personality.voice} voice)`);
            updatePersonalityButton(personalityType, 'Start Conversation', false);
            updatePersonalitySun(personalityType, '');
            
            console.log(`üõë ${personalityType} conversation stopped cleanly`);
        }

        function bufferAudioChunk(personalityType, audioData) {
            const personality = personalities[personalityType];
            
            // Only buffer if we haven't played this response yet
            if (personality.hasPlayedResponse) {
                console.log(`‚è≠Ô∏è ${personalityType} skipping audio chunk - response already played`);
                return;
            }
            
            // Add to buffer
            personality.audioBuffer.push(audioData);
            console.log(`üîä ${personalityType} buffered chunk ${personality.audioBuffer.length}`);
            
            // Clear existing timeout
            if (personality.bufferTimeout) {
                clearTimeout(personality.bufferTimeout);
            }
            
            // If we have enough chunks or timeout, play the buffered audio
            if (personality.audioBuffer.length >= 3) {
                playBufferedAudio(personalityType);
            } else {
                // Set timeout to play buffered audio after 500ms
                personality.bufferTimeout = setTimeout(() => {
                    if (personality.audioBuffer.length > 0 && !personality.hasPlayedResponse) {
                        playBufferedAudio(personalityType);
                    }
                }, 500);
            }
        }

        function flushAudioBuffer(personalityType) {
            const personality = personalities[personalityType];
            
            // Only flush if we haven't played this response yet
            if (personality.hasPlayedResponse) {
                console.log(`‚è≠Ô∏è ${personalityType} skipping flush - response already played`);
                return;
            }
            
            if (personality.bufferTimeout) {
                clearTimeout(personality.bufferTimeout);
                personality.bufferTimeout = null;
            }
            
            if (personality.audioBuffer.length > 0) {
                console.log(`üéµ ${personalityType} flushing remaining ${personality.audioBuffer.length} audio chunks`);
                playBufferedAudio(personalityType);
            }
        }

        async function playBufferedAudio(personalityType) {
            const personality = personalities[personalityType];
            
            // Prevent playing if already played this response
            if (personality.hasPlayedResponse) {
                console.log(`‚è≠Ô∏è ${personalityType} skipping playback - response already played`);
                return;
            }
            
            if (personality.audioBuffer.length === 0) return;
            
            // Mark as played immediately to prevent loops
            personality.hasPlayedResponse = true;
            console.log(`üéµ ${personalityType} playing buffered audio from ${personality.audioBuffer.length} chunks (ONCE ONLY)`);
            
            // Clear timeout
            if (personality.bufferTimeout) {
                clearTimeout(personality.bufferTimeout);
                personality.bufferTimeout = null;
            }
            
            // Store buffer locally and clear the main buffer immediately
            const audioChunks = [...personality.audioBuffer];
            personality.audioBuffer = [];
            
            try {
                // Play each chunk sequentially with Web Audio API
                for (let i = 0; i < audioChunks.length; i++) {
                    const audioData = audioChunks[i];
                    await playAudioChunk(personalityType, audioData);
                }
                
                // Return to listening state only if this personality is still active
                if (currentActivePersonality === personalityType) {
                    personality.responseState = 'listening';
                    updatePersonalityStatus(personalityType, 'Listening...');
                    updatePersonalitySun(personalityType, 'listening');
                }
                
                console.log(`üéµ ${personalityType} finished playing response - ready for next user input`);
                
            } catch (error) {
                console.error(`‚ùå ${personalityType} audio playback error:`, error);
                personality.hasPlayedResponse = false; // Reset on error
            }
        }

        async function playAudioChunk(personalityType, audioData) {
            const personality = personalities[personalityType];
            
            try {
                // Resume audio context if needed
                if (audioContext.state === 'suspended') {
                    await audioContext.resume();
                }
                
                // Decode base64 audio data
                const binaryString = atob(audioData);
                const bytes = new Uint8Array(binaryString.length);
                for (let i = 0; i < binaryString.length; i++) {
                    bytes[i] = binaryString.charCodeAt(i);
                }
                
                // Decode audio buffer
                const audioBuffer = await audioContext.decodeAudioData(bytes.buffer);
                
                // Create audio source
                const source = audioContext.createBufferSource();
                source.buffer = audioBuffer;
                source.connect(audioContext.destination);
                
                // Schedule playback
                const startTime = Math.max(audioContext.currentTime, personality.nextStartTime);
                source.start(startTime);
                
                // Update next start time for seamless playback
                personality.nextStartTime = startTime + audioBuffer.duration;
                
                console.log(`üéµ ${personalityType} audio scheduled seamlessly at ${startTime.toFixed(3)}s`);
                
                // Store current source for stopping if needed
                personality.currentSource = source;
                
                return new Promise((resolve) => {
                    source.onended = () => {
                        resolve();
                    };
                });
                
            } catch (error) {
                console.error(`‚ùå ${personalityType} audio playback error:`, error);
            }
        }

        function stopPersonalityAudio(personalityType) {
            const personality = personalities[personalityType];
            
            // Stop current audio source
            if (personality.currentSource) {
                try {
                    personality.currentSource.stop();
                } catch (error) {
                    // Source might already be stopped
                }
                personality.currentSource = null;
            }
            
            // Clear audio buffer and timeout
            personality.audioBuffer = [];
            if (personality.bufferTimeout) {
                clearTimeout(personality.bufferTimeout);
                personality.bufferTimeout = null;
            }
            
            // Reset next start time and response state
            personality.nextStartTime = 0;
            personality.responseState = 'listening';
            personality.hasPlayedResponse = false;
            
            console.log(`üîá ${personalityType} audio stopped and state reset`);
        }

        function stopAllConversations() {
            console.log('üõë Stopping all conversations');
            
            Object.keys(personalities).forEach(type => {
                if (personalities[type].isConnected) {
                    stopPersonalityConversation(type);
                }
            });
            
            currentActivePersonality = null;
            updatePersonalityCardStates();
            updateGlobalStatus('All conversations stopped');
        }

        // UI Update Functions
        function updateGlobalStatus(message) {
            document.getElementById('globalStatus').textContent = message;
        }

        function updatePersonalityStatus(personalityType, message) {
            document.getElementById(`status-${personalityType}`).textContent = message;
        }

        function updatePersonalityButton(personalityType, text, disabled) {
            const button = document.getElementById(`btn-${personalityType}`);
            button.textContent = text;
            button.disabled = disabled;
            
            if (personalities[personalityType].isConnected) {
                button.classList.add('active');
            } else {
                button.classList.remove('active');
            }
        }

        function updatePersonalitySun(personalityType, state) {
            const sun = document.getElementById(`sun-${personalityType}`);
            sun.className = 'sun';
            if (state) {
                sun.classList.add(state);
            }
        }

        function updatePersonalityCardStates() {
            Object.keys(personalities).forEach(type => {
                const card = document.getElementById(`card-${type}`);
                
                if (currentActivePersonality === type) {
                    card.classList.add('active');
                    card.classList.remove('inactive');
                } else if (currentActivePersonality !== null) {
                    card.classList.add('inactive');
                    card.classList.remove('active');
                } else {
                    card.classList.remove('active', 'inactive');
                }
            });
        }

        function addToPersonalityTranscript(personalityType, message) {
            const transcript = document.getElementById(`transcript-${personalityType}`);
            const timestamp = new Date().toLocaleTimeString();
            transcript.innerHTML += `<div><small>${timestamp}</small> ${message}</div>`;
            transcript.scrollTop = transcript.scrollHeight;
        }
    </script>
</body>
</html>






